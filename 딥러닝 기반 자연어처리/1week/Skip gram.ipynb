{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Skip gram.ipynb","provenance":[],"authorship_tag":"ABX9TyMZBS6QRZEIga/cSWKklNLy"},"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit ('nlp': conda)"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"8996e42b5752b24954ef4ae74940c33bc999c8a2dee14898c711298040aa64c3"}},"cells":[{"cell_type":"code","execution_count":12,"source":["import tensorflow as tf\n","import numpy as np"],"outputs":[],"metadata":{"id":"TJ_LnEuYGAEI","executionInfo":{"status":"ok","timestamp":1626754084991,"user_tz":-540,"elapsed":10,"user":{"displayName":"홍경수","photoUrl":"","userId":"10514684854108920109"}}}},{"cell_type":"code","execution_count":2,"source":["corpus_raw = \"He is the king . The king is royal . She is the royal queen\""],"outputs":[],"metadata":{"id":"uccpplK0GSsw","executionInfo":{"status":"ok","timestamp":1626754142954,"user_tz":-540,"elapsed":395,"user":{"displayName":"홍경수","photoUrl":"","userId":"10514684854108920109"}}}},{"cell_type":"code","execution_count":6,"source":["raw_sentence = corpus_raw.split(\".\")\n","sentences = []\n","for sentence in raw_sentence:\n","    sentences.append(sentence.strip().split())\n","\n","sentences"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['He', 'is', 'the', 'king'],\n"," ['The', 'king', 'is', 'royal'],\n"," ['She', 'is', 'the', 'royal', 'queen']]"]},"metadata":{},"execution_count":6}],"metadata":{"id":"I2eJoFc-Gk9g"}},{"cell_type":"code","execution_count":7,"source":["data = []\n","WINDOW_SIZE = 2\n","\n","for sentence in sentences:\n","    for word_index, word in enumerate(sentence):\n","        start_index = max(word_index - WINDOW_SIZE, 0)\n","        end_index = min(word_index + WINDOW_SIZE + 1, len(sentence))\n","\n","        for nb_word in sentence[start_index:word_index]:\n","            data.append([word, nb_word])\n","\n","        for nb_word in sentence[word_index + 1 : end_index]:\n","            data.append([word, nb_word])\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["data"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['He', 'is'],\n"," ['He', 'the'],\n"," ['is', 'He'],\n"," ['is', 'the'],\n"," ['is', 'king'],\n"," ['the', 'He'],\n"," ['the', 'is'],\n"," ['the', 'king'],\n"," ['king', 'is'],\n"," ['king', 'the'],\n"," ['The', 'king'],\n"," ['The', 'is'],\n"," ['king', 'The'],\n"," ['king', 'is'],\n"," ['king', 'royal'],\n"," ['is', 'The'],\n"," ['is', 'king'],\n"," ['is', 'royal'],\n"," ['royal', 'king'],\n"," ['royal', 'is'],\n"," ['She', 'is'],\n"," ['She', 'the'],\n"," ['is', 'She'],\n"," ['is', 'the'],\n"," ['is', 'royal'],\n"," ['the', 'She'],\n"," ['the', 'is'],\n"," ['the', 'royal'],\n"," ['the', 'queen'],\n"," ['royal', 'is'],\n"," ['royal', 'the'],\n"," ['royal', 'queen'],\n"," ['queen', 'the'],\n"," ['queen', 'royal']]"]},"metadata":{},"execution_count":8}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["words = []\n","for word in corpus_raw.split():\n","    if word != \".\":\n","        words.append(word)\n","words = set(words)\n","\n","word2int = {}\n","int2word = {}\n","vocab_size = len(words)\n","\n","for i, word in enumerate(words):\n","    word2int[word] = i\n","    int2word[i] = word\n","\n","print(word2int)\n","print(int2word)"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'She': 0, 'He': 1, 'king': 2, 'is': 3, 'queen': 4, 'the': 5, 'royal': 6, 'The': 7}\n","{0: 'She', 1: 'He', 2: 'king', 3: 'is', 4: 'queen', 5: 'the', 6: 'royal', 7: 'The'}\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["def to_one_hot(word_index, vocab_size):\n","    temp = np.zeros(vocab_size)\n","    temp[word_index] = 1\n","    return temp"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["x_train = []\n","y_train = []\n","for words in data:\n","    x_train.append(to_one_hot(word2int[words[0]], vocab_size))\n","    y_train.append(to_one_hot(word2int[words[1]], vocab_size))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["x_train[:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([0., 1., 0., 0., 0., 0., 0., 0.]),\n"," array([0., 1., 0., 0., 0., 0., 0., 0.]),\n"," array([0., 0., 0., 1., 0., 0., 0., 0.])]"]},"metadata":{},"execution_count":15}],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["y_train[:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([0., 0., 0., 1., 0., 0., 0., 0.]),\n"," array([0., 0., 0., 0., 0., 1., 0., 0.]),\n"," array([0., 1., 0., 0., 0., 0., 0., 0.])]"]},"metadata":{},"execution_count":16}],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["x_train = np.asarray(x_train, dtype=np.float32)\n","y_train = np.asarray(y_train, dtype=np.float32)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["x_train[:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":18}],"metadata":{}},{"cell_type":"code","execution_count":27,"source":["class Word2Vec:\n","    def __init__(self, vocab_size=10, embedding_dim=5, optimizer='sgd',\n","                 epochs=1000, learning_rate=0.01):\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.epochs = epochs\n","        if optimizer == 'adam':\n","            self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n","        else:\n","            self.optimizer = tf.optimizers.SGD(learning_rate=learning_rate)\n","        \n","\n","        self.W1 = tf.Variable(tf.random.normal([self.vocab_size, \n","                                                self.embedding_dim]))\n","        self.b1 = tf.Variable(tf.random.normal([self.embedding_dim]))\n","\n","        self.W2 = tf.Variable(tf.random.normal([self.embedding_dim,\n","                                                self.vocab_size]))\n","        self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n","\n","    def vectorized(self, word_index):\n","        return (self.W1 + self.b1)[word_index]\n","\n","\n","    def train(self, x_train, y_train):\n","        for i in range(self.epochs):\n","            with tf.GradientTape() as tape:\n","                hidden_layer = tf.add(tf.matmul(x_train, self.W1), self.b1)\n","                output_layer = tf.add(tf.matmul(hidden_layer, self.W2), self.b2)\n","\n","                pred = tf.nn.softmax(output_layer)\n","                loss = tf.reduce_mean(-tf.math.reduce_sum(y_train*tf.math.log(pred), axis=[1]))\n","\n","                params = [self.W1, self.b1, self.W2, self.b2]\n","                grads = tape.gradient(loss, params)\n","                self.optimizer.apply_gradients(zip(grads,params))\n","            if i % 1000 == 0:\n","                print(loss)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":28,"source":["w2v = Word2Vec(vocab_size=vocab_size, embedding_dim=5, optimizer='SGD', epochs=10000, learning_rate=0.1)\n","w2v.train(x_train, y_train)"],"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(3.7162118, shape=(), dtype=float32)\n","tf.Tensor(1.3957883, shape=(), dtype=float32)\n","tf.Tensor(1.3800758, shape=(), dtype=float32)\n","tf.Tensor(1.3773054, shape=(), dtype=float32)\n","tf.Tensor(1.3762228, shape=(), dtype=float32)\n","tf.Tensor(1.3756589, shape=(), dtype=float32)\n","tf.Tensor(1.3753175, shape=(), dtype=float32)\n","tf.Tensor(1.3750902, shape=(), dtype=float32)\n","tf.Tensor(1.374929, shape=(), dtype=float32)\n","tf.Tensor(1.3748091, shape=(), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Gensim"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import pandas as pd\n","df_news = pd.read_csv('./data/news.csv')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["df_news.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["         date   media                                 title  \\\n","0  2005-01-01  연합인포맥스               내년 美국채수익률 5% 넘어서기 어려울 듯   \n","1  2005-01-01  연합인포맥스  [뉴욕채권-마감] 10년만기 국채수익률 작년보다 낮은 수준서 마쳐   \n","2  2005-01-01  연합인포맥스       [뉴욕환시] `내년초 달러-엔에 주력'..달러 對엔 하락   \n","3  2005-01-01  연합인포맥스         [31일 뉴욕금융시장 요약] 한산한 거래속 새해 준비   \n","4  2005-01-02  연합인포맥스       美 증시 기술주 주도로 2년 연속 상승..'01년래 최고   \n","\n","                                             content  \\\n","0  2005년 10년만기 미국 국채수익률이  연방준비제도이사회(FRB)의 금리인상 지속...   \n","1  2년만기 국채가격 4년래 최악의 한해 보내     10년만기 미국  국채수익률이  ...   \n","2  2004년 마지막 거래일인 31일  뉴욕환시에서 미국 달러화는 개장초 102엔 근처...   \n","3  ) 한해 마지막 날인 31일 뉴욕 주요 금융시장은 한산한 거래속에 새해를 준비하는 ...   \n","4  지난해 뉴욕증시는 기술주 주도로 2년  연속연초 대비 상승하면서 대표지수들을 지난 ...   \n","\n","                                              ngrams  \n","0  만기/NNG,국채/NNG,수익률/NNG,fed/NNG,fed/NNG,금리/NNG,인...  \n","1  만기/NNG,국채/NNG,가격/NNG,최악/NNG,보내/VV,만기/NNG,국채/NN...  \n","2  마지막/NNG,거래일/NNG,뉴욕/NNG,환시/NNG,달러/NNG,개장/NNG,근처...  \n","3  마지막/NNG,뉴욕/NNG,금융시장/NNG,한산/NNG,거래/NNG,새해/NNG,준...  \n","4  주도/NNG,연속/NNG,대비/NNG,상승/NNG,대표지수/NNG,최고/NNG,오르...  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>media</th>\n","      <th>title</th>\n","      <th>content</th>\n","      <th>ngrams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2005-01-01</td>\n","      <td>연합인포맥스</td>\n","      <td>내년 美국채수익률 5% 넘어서기 어려울 듯</td>\n","      <td>2005년 10년만기 미국 국채수익률이  연방준비제도이사회(FRB)의 금리인상 지속...</td>\n","      <td>만기/NNG,국채/NNG,수익률/NNG,fed/NNG,fed/NNG,금리/NNG,인...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2005-01-01</td>\n","      <td>연합인포맥스</td>\n","      <td>[뉴욕채권-마감] 10년만기 국채수익률 작년보다 낮은 수준서 마쳐</td>\n","      <td>2년만기 국채가격 4년래 최악의 한해 보내     10년만기 미국  국채수익률이  ...</td>\n","      <td>만기/NNG,국채/NNG,가격/NNG,최악/NNG,보내/VV,만기/NNG,국채/NN...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2005-01-01</td>\n","      <td>연합인포맥스</td>\n","      <td>[뉴욕환시] `내년초 달러-엔에 주력'..달러 對엔 하락</td>\n","      <td>2004년 마지막 거래일인 31일  뉴욕환시에서 미국 달러화는 개장초 102엔 근처...</td>\n","      <td>마지막/NNG,거래일/NNG,뉴욕/NNG,환시/NNG,달러/NNG,개장/NNG,근처...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2005-01-01</td>\n","      <td>연합인포맥스</td>\n","      <td>[31일 뉴욕금융시장 요약] 한산한 거래속 새해 준비</td>\n","      <td>) 한해 마지막 날인 31일 뉴욕 주요 금융시장은 한산한 거래속에 새해를 준비하는 ...</td>\n","      <td>마지막/NNG,뉴욕/NNG,금융시장/NNG,한산/NNG,거래/NNG,새해/NNG,준...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2005-01-02</td>\n","      <td>연합인포맥스</td>\n","      <td>美 증시 기술주 주도로 2년 연속 상승..'01년래 최고</td>\n","      <td>지난해 뉴욕증시는 기술주 주도로 2년  연속연초 대비 상승하면서 대표지수들을 지난 ...</td>\n","      <td>주도/NNG,연속/NNG,대비/NNG,상승/NNG,대표지수/NNG,최고/NNG,오르...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":2}],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["df_news['content'][30000]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["'26일 오전 서울채권시장 국고채금리는 전주 미국채시장의 약세 영향으로 소폭 올라 출발한 후 재료 부족으로 정체됐다.    이날 국고5년 7-1 지표물 금리는 전주에서 1bp 오른 4.80%에서 개장한 후 4.81%로 올랐다.    전주 미국채 금리는 지난 2월 기존 주택판매가 예상 밖의 호조를 보여 상승했다.10년 만기 국채금리는 2bp 오른 연 4.61%에, 통화정책에 민감한 2년 만기 금리는 1bp 이상 높은 연 4.61%에 마감됐다.    연합인포맥스가 지난 19일부터 23일까지 17개 국내외 금융기관의 이코노미스트 들을 대상으로 실시한 폴에 따르면 2월 산업생산은 작년 같은 달에 비해 3.2%  증가할 것으로 집계됐다. 이 같은 컨센서스는 전월 증가율 7.4%보다 4.2%p 낮은 것으로 1월과 2월을 평균할 때는 5.3%를 보였다.    또 인포맥스가 같은 기간 국내 14개 경제연구소와 금융기관을 대상으로 설문조 사를 실시한 결과, 올해 1분기 GDP 증가율은 전년동기대비 3.87%로 예상됐다.    아울러 인포맥스가 같은 기간 14개 국내외 금융기관 및 경제연구소 이코노미스 트들을 대상으로 설문조사를 실시한 결과 3월 CPI는 전년동월대비 2.2% 상승할 것으로 추정됐다.    또 각종 입찰이 있었다.    한국은행은 2일 만기 5조원 규모 RP 매각 입찰에서 6조8천억원이  몰려,  5조원 전액이 연 4.57%에 낙찰됐다고 발표했다.    재정경제부는 국고20년 3천770억원 입찰에서 8천140억원이 응찰을 해와 전액이 연 5.14% 낙찰됐다고 밝혔다.    은행권 채권딜러는 \"국고20년물이 시장 예상치 보다 1bp 정도 낮게 낙찰돼 시장분위기가 좀 강해질 수 있다\"며 \"하지만 장중 금리가 반락할 것 같지는 않다\"고  예상했다.    이 딜러는 \"국고20년 금리가 그 동안 다른 짧은 기간물에 비해 많이 올라서  상대적으로 입찰 때 매수가 강했다\"며 \"또 단기금리가 탄탄한 하방경직성을 보여서 금리 하락시도를 하기가 버겁다\"고 덧붙였다.    liberte@yna.co.kr26일 오전 서울채권시장 국고채금리는 전주 미국채시장의 약세 영향으로 소폭 올라 출발한 후 재료 부족으로 정체됐다.    이날 국고5년 7-1 지표물 금리는 전주에서 1bp 오른 4.80%에서 개장한 후 4.81%로 올랐다.    전주 미국채 금리는 지난 2월 기존 주택판매가 예상 밖의 호조를 보여 상승했다.10년 만기 국채금리는 2bp 오른 연 4.61%에, 통화정책에 민감한 2년 만기 금리는 1bp 이상 높은 연 4.61%에 마감됐다.    연합인포맥스가 지난 19일부터 23일까지 17개 국내외 금융기관의 이코노미스트 들을 대상으로 실시한 폴에 따르면 2월 산업생산은 작년 같은 달에 비해 3.2%  증가할 것으로 집계됐다. 이 같은 컨센서스는 전월 증가율 7.4%보다 4.2%p 낮은 것으로 1월과 2월을 평균할 때는 5.3%를 보였다.    또 인포맥스가 같은 기간 국내 14개 경제연구소와 금융기관을 대상으로 설문조 사를 실시한 결과, 올해 1분기 GDP 증가율은 전년동기대비 3.87%로 예상됐다.    아울러 인포맥스가 같은 기간 14개 국내외 금융기관 및 경제연구소 이코노미스 트들을 대상으로 설문조사를 실시한 결과 3월 CPI는 전년동월대비 2.2% 상승할 것으로 추정됐다.    또 각종 입찰이 있었다.    한국은행은 2일 만기 5조원 규모 RP 매각 입찰에서 6조8천억원이  몰려,  5조원 전액이 연 4.57%에 낙찰됐다고 발표했다.    재정경제부는 국고20년 3천770억원 입찰에서 8천140억원이 응찰을 해와 전액이 연 5.14% 낙찰됐다고 밝혔다.    은행권 채권딜러는 \"국고20년물이 시장 예상치 보다 1bp 정도 낮게 낙찰돼 시장분위기가 좀 강해질 수 있다\"며 \"하지만 장중 금리가 반락할 것 같지는 않다\"고  예상했다.    이 딜러는 \"국고20년 금리가 그 동안 다른 짧은 기간물에 비해 많이 올라서  상대적으로 입찰 때 매수가 강했다\"며 \"또 단기금리가 탄탄한 하방경직성을 보여서 금리 하락시도를 하기가 버겁다\"고 덧붙였다.    liberte@yna.co.kr'"]},"metadata":{},"execution_count":3}],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["sentences = df_news['content'].apply(lambda x : x.split('.    ')).tolist()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["sentences[0]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['2005년 10년만기 미국 국채수익률이  연방준비제도이사회(FRB)의 금리인상 지속에 따른 인플레이션 압력 완화로 연 5%를 넘어서기 어려울 것으로 전망됐다',\n"," '31일 씨티그룹 애널리스트들은 2005년에 미국의 인플레가 잘 제어될 것이라면서반면 내년 2년만기 국채수익률은 FRB의 지속적인 금리인상으로 연 4.00-4.50%  수준까지 상승하게 될 것이라고 예측했다',\n"," '씨티그룹은 단기 국채수익률이 상승세를 나타낼 것으로 보이는 반면 장기  국채수익률의 상승폭은 제한될 것으로 보여 수익률 곡선 평탄화가 가속화될 것이라고 덧붙였다',\n"," '씨티그룹은 내년 고용창출 호조가 가구당 수입증가를 견인할 것이라면서 고용시장 호전이 소비자지출을 떠받치게 될 것이라고 말했다']"]},"metadata":{},"execution_count":6}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["len(sentences)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["261817"]},"metadata":{},"execution_count":8}],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["from konlpy.tag import Mecab\n","mecab = Mecab()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["corpus = [mecab.morphs(sent) for para in sentences for sent in para]"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["from gensim.models import Word2Vec\n","model = Word2Vec(sentences=corpus, \n","                 vector_size=100, \n","                 alpha=0.025, \n","                 window=5, \n","                 min_count=1,\n","                 workers=4)"],"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-420c30d9d70b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = Word2Vec(sentences=corpus, \n\u001b[0m\u001b[1;32m      3\u001b[0m                  \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1063\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m   1424\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["word_embedding = model.wv['금리']"],"outputs":[{"output_type":"error","ename":"KeyError","evalue":"\"Key '금리' not present\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-7805f86d0d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'금리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Key '금리' not present\""]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}