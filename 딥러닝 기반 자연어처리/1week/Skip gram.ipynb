{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Skip gram.ipynb","provenance":[],"authorship_tag":"ABX9TyMZBS6QRZEIga/cSWKklNLy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"source":["import tensorflow as tf\n","import numpy as np"],"outputs":[],"metadata":{"id":"TJ_LnEuYGAEI","executionInfo":{"status":"ok","timestamp":1626754084991,"user_tz":-540,"elapsed":10,"user":{"displayName":"홍경수","photoUrl":"","userId":"10514684854108920109"}}}},{"cell_type":"code","execution_count":2,"source":["corpus_raw = \"He is the king . The king is royal . She is the royal queen\""],"outputs":[],"metadata":{"id":"uccpplK0GSsw","executionInfo":{"status":"ok","timestamp":1626754142954,"user_tz":-540,"elapsed":395,"user":{"displayName":"홍경수","photoUrl":"","userId":"10514684854108920109"}}}},{"cell_type":"code","execution_count":6,"source":["raw_sentence = corpus_raw.split(\".\")\n","sentences = []\n","for sentence in raw_sentence:\n","    sentences.append(sentence.strip().split())\n","\n","sentences"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['He', 'is', 'the', 'king'],\n"," ['The', 'king', 'is', 'royal'],\n"," ['She', 'is', 'the', 'royal', 'queen']]"]},"metadata":{},"execution_count":6}],"metadata":{"id":"I2eJoFc-Gk9g"}},{"cell_type":"code","execution_count":7,"source":["data = []\n","WINDOW_SIZE = 2\n","\n","for sentence in sentences:\n","    for word_index, word in enumerate(sentence):\n","        start_index = max(word_index - WINDOW_SIZE, 0)\n","        end_index = min(word_index + WINDOW_SIZE + 1, len(sentence))\n","\n","        for nb_word in sentence[start_index:word_index]:\n","            data.append([word, nb_word])\n","\n","        for nb_word in sentence[word_index + 1 : end_index]:\n","            data.append([word, nb_word])\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["data"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['He', 'is'],\n"," ['He', 'the'],\n"," ['is', 'He'],\n"," ['is', 'the'],\n"," ['is', 'king'],\n"," ['the', 'He'],\n"," ['the', 'is'],\n"," ['the', 'king'],\n"," ['king', 'is'],\n"," ['king', 'the'],\n"," ['The', 'king'],\n"," ['The', 'is'],\n"," ['king', 'The'],\n"," ['king', 'is'],\n"," ['king', 'royal'],\n"," ['is', 'The'],\n"," ['is', 'king'],\n"," ['is', 'royal'],\n"," ['royal', 'king'],\n"," ['royal', 'is'],\n"," ['She', 'is'],\n"," ['She', 'the'],\n"," ['is', 'She'],\n"," ['is', 'the'],\n"," ['is', 'royal'],\n"," ['the', 'She'],\n"," ['the', 'is'],\n"," ['the', 'royal'],\n"," ['the', 'queen'],\n"," ['royal', 'is'],\n"," ['royal', 'the'],\n"," ['royal', 'queen'],\n"," ['queen', 'the'],\n"," ['queen', 'royal']]"]},"metadata":{},"execution_count":8}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["words = []\n","for word in corpus_raw.split():\n","    if word != \".\":\n","        words.append(word)\n","words = set(words)\n","\n","word2int = {}\n","int2word = {}\n","vocab_size = len(words)\n","\n","for i, word in enumerate(words):\n","    word2int[word] = i\n","    int2word[i] = word\n","\n","print(word2int)\n","print(int2word)"],"outputs":[{"output_type":"stream","name":"stdout","text":["{'She': 0, 'He': 1, 'king': 2, 'is': 3, 'queen': 4, 'the': 5, 'royal': 6, 'The': 7}\n","{0: 'She', 1: 'He', 2: 'king', 3: 'is', 4: 'queen', 5: 'the', 6: 'royal', 7: 'The'}\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["def to_one_hot(word_index, vocab_size):\n","    temp = np.zeros(vocab_size)\n","    temp[word_index] = 1\n","    return temp"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["x_train = []\n","y_train = []\n","for words in data:\n","    x_train.append(to_one_hot(word2int[words[0]], vocab_size))\n","    y_train.append(to_one_hot(word2int[words[1]], vocab_size))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["x_train[:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([0., 1., 0., 0., 0., 0., 0., 0.]),\n"," array([0., 1., 0., 0., 0., 0., 0., 0.]),\n"," array([0., 0., 0., 1., 0., 0., 0., 0.])]"]},"metadata":{},"execution_count":15}],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["y_train[:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([0., 0., 0., 1., 0., 0., 0., 0.]),\n"," array([0., 0., 0., 0., 0., 1., 0., 0.]),\n"," array([0., 1., 0., 0., 0., 0., 0., 0.])]"]},"metadata":{},"execution_count":16}],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["x_train = np.asarray(x_train, dtype=np.float32)\n","y_train = np.asarray(y_train, dtype=np.float32)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["x_train[:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":18}],"metadata":{}},{"cell_type":"code","execution_count":27,"source":["class Word2Vec:\n","    def __init__(self, vocab_size=10, embedding_dim=5, optimizer='sgd',\n","                 epochs=1000, learning_rate=0.01):\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.epochs = epochs\n","        if optimizer == 'adam':\n","            self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n","        else:\n","            self.optimizer = tf.optimizers.SGD(learning_rate=learning_rate)\n","        \n","\n","        self.W1 = tf.Variable(tf.random.normal([self.vocab_size, \n","                                                self.embedding_dim]))\n","        self.b1 = tf.Variable(tf.random.normal([self.embedding_dim]))\n","\n","        self.W2 = tf.Variable(tf.random.normal([self.embedding_dim,\n","                                                self.vocab_size]))\n","        self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n","\n","    def vectorized(self, word_index):\n","        return (self.W1 + self.b1)[word_index]\n","\n","\n","    def train(self, x_train, y_train):\n","        for i in range(self.epochs):\n","            with tf.GradientTape() as tape:\n","                hidden_layer = tf.add(tf.matmul(x_train, self.W1), self.b1)\n","                output_layer = tf.add(tf.matmul(hidden_layer, self.W2), self.b2)\n","\n","                pred = tf.nn.softmax(output_layer)\n","                loss = tf.reduce_mean(-tf.math.reduce_sum(y_train*tf.math.log(pred), axis=[1]))\n","\n","                params = [self.W1, self.b1, self.W2, self.b2]\n","                grads = tape.gradient(loss, params)\n","                self.optimizer.apply_gradients(zip(grads,params))\n","            if i % 1000 == 0:\n","                print(loss)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":28,"source":["w2v = Word2Vec(vocab_size=vocab_size, embedding_dim=5, optimizer='SGD', epochs=10000, learning_rate=0.1)\n","w2v.train(x_train, y_train)"],"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(3.7162118, shape=(), dtype=float32)\n","tf.Tensor(1.3957883, shape=(), dtype=float32)\n","tf.Tensor(1.3800758, shape=(), dtype=float32)\n","tf.Tensor(1.3773054, shape=(), dtype=float32)\n","tf.Tensor(1.3762228, shape=(), dtype=float32)\n","tf.Tensor(1.3756589, shape=(), dtype=float32)\n","tf.Tensor(1.3753175, shape=(), dtype=float32)\n","tf.Tensor(1.3750902, shape=(), dtype=float32)\n","tf.Tensor(1.374929, shape=(), dtype=float32)\n","tf.Tensor(1.3748091, shape=(), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}