{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLGhbEiOoAR7"
   },
   "source": [
    "# 텍스트 전처리 (Text Preprocessing)\n",
    "\n",
    "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
    "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2V9xT1ADXodF"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXjgGKs_XwkE",
    "outputId": "b568f148-21ab-48cb-989e-1697b512c4d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EF'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = Mecab()\n",
    "mecab.pos(\"아버지가방에들어가신다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E585k45HDx5E"
   },
   "source": [
    "### 1) 토큰화 (Tokenizing)\n",
    "* 텍스트를 자연어 처리를 위해 분리 하는 것을\n",
    "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
    "\n",
    "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭하도록 한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "senwNSwgDzQc"
   },
   "source": [
    "### 2) 품사 부착(PoS Tagging)\n",
    "* 각 토큰에 품사 정보를 추가\n",
    "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R15ri5czDyzc"
   },
   "source": [
    "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
    "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
    "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
    "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfq99EkzD1Tk"
   },
   "source": [
    "### 4) 원형 복원 (Stemming & Lemmatization)\n",
    "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
    "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
    "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5HQOjRvDxmd"
   },
   "source": [
    "### 5) 불용어 처리 (Stopword)\n",
    "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
    "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
    "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaIYJczuaS0n"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KysKAL3VlgQN"
   },
   "source": [
    "# 1 영문 전처리 실습\n",
    "\n",
    "\n",
    "NLTK lib (https://www.nltk.org/) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mND0us3Jppcu"
   },
   "source": [
    "## 1.1 실습용 영문기사 수집\n",
    "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
    "\n",
    "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V6uXmtvpdEYI"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uNlsYqR2c65T"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/\"\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = soup.select(\"p\")\n",
    "text = article[3].get_text()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv0ASXb8qa6H"
   },
   "source": [
    "## 1.2 영문 토큰화\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daydream/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(\"Good muffins cost $3.88\\nin New York.\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokens = WordPunctTokenizer().tokenize(\"Good muffins cost $3.88\\nin New York.\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "word_tokens = TreebankWordTokenizer().tokenize(\"Good muffins cost $3.88\\nin New York.\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Z-0Nnysqnq"
   },
   "source": [
    "## 1.3 영문 품사 부착 (PoS Tagging)\n",
    "분리한 토큰마다 품사를 부착한다\n",
    "\n",
    "https://www.nltk.org/api/nltk.tag.html\n",
    "\n",
    "태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "N_RfsPif9R5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daydream/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged  = pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDo-5-khs5Oz"
   },
   "source": [
    "## 1.4 개체명 인식 (NER, Named Entity Recognition)\n",
    "\n",
    "http://www.nltk.org/api/nltk.chunk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "X7HePfSA9R5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/daydream/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/daydream/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"maxent_ne_chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Good/JJ)\n",
      "  muffins/NNS\n",
      "  cost/VBP\n",
      "  $/$\n",
      "  3.88/CD\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "ne_token = ne_chunk(tagged)\n",
    "print(ne_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHjV0h0ZtM-t"
   },
   "source": [
    "## 1.5 원형 복원\n",
    "각 토큰의 원형을 복원하여 표준화 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2eCnbChtXjo"
   },
   "source": [
    "### 1.5.1 어간추출 (Stemming)\n",
    "\n",
    "* 규칙에 기반 하여 토큰을 표준화\n",
    "* ning제거, ful 제거 등\n",
    "\n",
    "https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lhD5Z6lb9R5g"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4haNWIcCtZza"
   },
   "source": [
    "### 1.5.2 표제어 추출 (Lemmatization)\n",
    "\n",
    "* 품사정보를 보존하여 토큰을 표준화\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/daydream/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4J7Pysd09R5g"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beutiful'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize('beutiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmY_SvDMb0fz"
   },
   "source": [
    "## 1.6 불용어 처리 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 3),\n",
       " (('from', 'IN'), 3),\n",
       " (('to', 'TO'), 3),\n",
       " (('and', 'CC'), 3),\n",
       " (('in', 'IN'), 3),\n",
       " (('.', '.'), 3),\n",
       " (('job', 'NN'), 2),\n",
       " (('the', 'DT'), 2),\n",
       " (('of', 'IN'), 2),\n",
       " (('will', 'MD'), 2),\n",
       " (('now', 'RB'), 2),\n",
       " (('all', 'DT'), 2),\n",
       " (('And', 'CC'), 1),\n",
       " (('yes', 'UH'), 1),\n",
       " (('she', 'PRP'), 1),\n",
       " (('does', 'VBZ'), 1),\n",
       " (('mean', 'VB'), 1),\n",
       " (('everybody', 'NN'), 1),\n",
       " ((\"'s\", 'POS'), 1),\n",
       " (('yours', 'NNS'), 1),\n",
       " (('mine', 'VB'), 1),\n",
       " (('onward', 'VB'), 1),\n",
       " (('role', 'NN'), 1),\n",
       " (('grain', 'NN'), 1),\n",
       " (('farmers', 'NNS'), 1),\n",
       " (('Egypt', 'NNP'), 1),\n",
       " (('pastry', 'NN'), 1),\n",
       " (('chefs', 'NNS'), 1),\n",
       " (('Paris', 'NNP'), 1),\n",
       " (('dog', 'NN'), 1),\n",
       " (('walkers', 'NNS'), 1),\n",
       " (('Oregon', 'NNP'), 1),\n",
       " (('i.e', 'NN'), 1),\n",
       " (('every', 'DT'), 1),\n",
       " (('We', 'PRP'), 1),\n",
       " (('be', 'VB'), 1),\n",
       " (('able', 'JJ'), 1),\n",
       " (('help', 'VB'), 1),\n",
       " (('direct', 'VB'), 1),\n",
       " (('workers', 'NNS'), 1),\n",
       " (('’', 'VBP'), 1),\n",
       " (('actions', 'NNS'), 1),\n",
       " (('behavior', 'NN'), 1),\n",
       " (('with', 'IN'), 1),\n",
       " (('a', 'DT'), 1),\n",
       " (('new', 'JJ'), 1),\n",
       " (('degree', 'NN'), 1),\n",
       " (('intelligence', 'NN'), 1),\n",
       " (('that', 'WDT'), 1),\n",
       " (('comes', 'VBZ'), 1),\n",
       " (('predictive', 'JJ'), 1),\n",
       " (('analytics', 'NNS'), 1),\n",
       " (('stemming', 'VBG'), 1),\n",
       " (('AI', 'NNP'), 1),\n",
       " (('engines', 'VBZ'), 1),\n",
       " (('we', 'PRP'), 1),\n",
       " (('increasingly', 'RB'), 1),\n",
       " (('depend', 'VBP'), 1),\n",
       " (('upon', 'NN'), 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tagged  = pos_tag(word_tokenize(text))\n",
    "Counter(tagged).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " ',',\n",
       " 'she',\n",
       " 'does',\n",
       " 'mean',\n",
       " 'everybody',\n",
       " \"'s\",\n",
       " 'job',\n",
       " 'yours',\n",
       " 'to',\n",
       " 'mine',\n",
       " 'onward',\n",
       " 'to',\n",
       " 'role',\n",
       " 'grain',\n",
       " 'farmers',\n",
       " 'Egypt',\n",
       " ',',\n",
       " 'pastry',\n",
       " 'chefs',\n",
       " 'Paris',\n",
       " 'dog',\n",
       " 'walkers',\n",
       " 'Oregon',\n",
       " 'i.e',\n",
       " '.',\n",
       " 'job',\n",
       " '.',\n",
       " 'We',\n",
       " 'will',\n",
       " 'now',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'help',\n",
       " 'direct',\n",
       " 'workers',\n",
       " '’',\n",
       " 'actions',\n",
       " 'behavior',\n",
       " 'new',\n",
       " 'degree',\n",
       " 'intelligence',\n",
       " 'that',\n",
       " 'comes',\n",
       " 'predictive',\n",
       " 'analytics',\n",
       " ',',\n",
       " 'stemming',\n",
       " 'AI',\n",
       " 'engines',\n",
       " 'we',\n",
       " 'will',\n",
       " 'now',\n",
       " 'increasingly',\n",
       " 'depend',\n",
       " 'upon',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_pos = ['IN', 'CC', 'DT']\n",
    "words = []\n",
    "for tag in tagged:\n",
    "    if not tag[1] in stop_pos:\n",
    "        words.append(tag[0])\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV0orUsOb6wD"
   },
   "source": [
    "## 1.7 영문 텍스트 전처리 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA7ewXNW9R5h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMErzPcbuYEa"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0Dhqm4zkHXl"
   },
   "source": [
    "# 2 한글 전처리 실습\n",
    "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIkGxDnNimek"
   },
   "source": [
    "## 2.1 실습용 한글기사 수집\n",
    "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
    "\n",
    "http://news.chosun.com/site/data/html_dir/2018/07/10/2018071004121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "iDK1f7ft9R5h"
   },
   "outputs": [],
   "source": [
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=110&oid=023&aid=0003386456'\n",
    "headers = {'user-agent':'Mozilla/5.0'}\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "// flash 오류를 우회하기 위한 함수 추가\n",
      "function _flash_removeCallback() {}\n",
      "\n",
      "美 IT 기업 창업자 가운데 컴퓨터·공학 전공은 37%뿐… 예술·인문학 등 전공 다양工學·예술의 창의적 협력으로 '혁신의 규칙'을 바꾸는 것이 4차 산업혁명의 시대정신비벡 와드와 카네기멜런대 교수·하버드대 로스쿨 특별연구원교육을 삶의 최우선 순위로 두고 있는 한국의 부모들은 대학 전공 가운데 의학과 공학·과학을 중시한다. 자녀의 직업적 성공을 위해 대학 전공으로 의학과 이공계를 우선적으로 고려하는 일은 한국이 산업화 중이던 상황에선 올바른 선택이었다. 하지만 지금은 모든 것이 달라졌다. 요즘 실리콘밸리에서 확인되는 것은 4차 산업혁명 시대에는 예술과 인문학이 의학·공학만큼 중요하다는 사실이다.스티브 잡스는 자신이 대학 시절 수강했던 서체(書體) 수업이 매킨토시(애플이 1984년 발표한 개인용 컴퓨터) 개발 성공에 큰 영향을 미쳤다고 말했다. 그는 2011년 아이패드 2를 공개하면서 \"애플의 DNA는 기술만으로는 충분하지 않다. 교양과 인문학이 결합한 기술이야말로 가슴 벅찬 결과를 낳을 것\"이라며 예술과 디자인의 중요성을 강조했다. 이런 관점을 바탕으로 잡스는 세계 최고 가치를 인정받는 기업을 만들었고, 기술 산업의 새로운 표준까지 정했다.실리콘밸리에서 최근 뜨고 있는 스타 기업인 중에는 인문학 전공자들이 제법 많다. 구인·구직 소셜 네트워킹 서비스 기업인 링크드인(LinkedIn) 창업자 리드 호프만은 철학 석사 학위 소지자이며, 수잔 보이치키 유튜브 CEO는 역사와 문학을 전공했다. 메신저 개발 업체 슬랙(Slack)의 창업자 스튜어트 버터필드는 철학, 세계 최대 숙박 공유 기업인 에어비앤비의 설립자 브라이언 체스키는 미술을 전공했다. 중국 알리바바그룹의 마윈 회장의 학부 전공은 영어였다.내가 속해 있는 하버드대·듀크대 연구팀은 미국 IT 기업 창업자들의 92%가 학사 학위를, 47%는 석사 학위 이상을 갖고 있음을 밝혀냈다. 창업자들의 세부 전공을 보면 37%만 공학·컴퓨터 기술이며, 수학 전공자는 2%뿐이었다. 이들의 전공은 경영·회계·보건·예술·인문학 등 매우 다양했다.컴퓨터 주변기기 제조 업체 로지텍의 브랙큰 대럴 CEO도 영문학을 전공했다. 최근 내가 그에게 \"어떻게 5년여 만에 회사 주가를 450% 올릴 수 있었느냐\"고 물었더니 그는 \"우리 회사가 만드는 모든 제품의 디자인을 쉼 없이 고민했기 때문에 가능했다. 기술 관련 제품의 성공에 가장 중요한 것은 디자인\"이라고 답했다.4차 산업혁명은 '혁신의 규칙'을 바꾸는 일이다. 여기에는 컴퓨터와 인공지능, 디지털 의술, 로봇공학, 합성생물학 등 광범위한 기술이 활용된다. 의학·인공지능과 센서를 융합하면 인간의 건강을 진단하고 질병을 예방하는 '디지털 의사'를 만들 수 있다. 유전체학과 유전자 편집을 이용해 가뭄에 강하고 인류 전체를 먹여 살릴 새 식물을 개발할 수 있다. 인공지능 로봇을 사용해 노인들을 위한 '디지털 친구'도 가능하다. 초소형 물질의 발전은 모든 사람이 충분히 이용할 수 있는 '태양열 저장 기술'의 새 시대를 열 것이다.이런 해결책을 이끌어내는 데는 생물학·교육·의료·인간행동 등 여러 분야에 대한 지식이 필요하다. 우리가 처한 거대한 사회·기술적 도전에 대처하려면 인류를 둘러싼 다양한 배경과 맥락에 대해 비판적 사고 능력이 필수적이다. 이는 인문학 전공자들이 가장 잘 훈련받은 분야이다.일례로 음악·예술·문학과 심리학에서 비롯한 공감(共感) 능력은 디자인에 큰 장점이 된다. 로마제국의 흥망성쇠와 계몽주의를 공부한 역사학도는 기술의 인간적 요소와 유용성에 대한 통찰력을 가질 수 있다. 심리학 전공자는 사람에게 동기(動機)부여가 무엇이며, 사용자들이 원하는 게 무엇인지를 공학 한 분야에서만 일해온 엔지니어들보다 잘 이해할 수 있다. 우리가 상상하는 물건을 3D로 만들 수 있게 되면 음악가나 화가들의 세상이 될지도 모른다.\"자녀가 미래에 어떤 직업을 택하면 좋겠는가\"라는 질문을 받을 때 나는 \"아이들이 스스로 하고 싶은 것을 선택하도록 내버려 두는 게 최선\"이라고 답한다. 과거 우리 부모들이 우리에게 공부를 강요해서 학습을 '억지로 해야 하는 따분한 일'로 여기게 했던 방식을 따라 해선 안 된다. 그 대신 자녀가 자신의 열정을 추구하고 배우는 걸 즐길 수 있도록 북돋아야 한다.기술을 통해 놀라운 미래를 창조하고 새로운 산업혁명 시대에 리더가 되기 위해 한국에서는 공학자들과 손잡고 일할 음악가와 화가가 더 많이 필요하다. 이제는 과학·공학·의학뿐 아니라 창의성과 디자인에도 진정한 관심을 쏟아야 한다.[비벡 와드와 카네기멜런대 교수·하버드대 로스쿨 특별연구원] [조선닷컴 바로가기][조선일보 구독신청하기] - Copyrights ⓒ 조선일보 & chosun.com, 무단 전재 및 재배포 금지 -\n",
      "\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article = soup.select(\"#articleBodyContents\")[0].text\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w09FHRgIphw5"
   },
   "source": [
    "## 2.2 한글 토큰화 및 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WXWgZVm49R5h"
   },
   "outputs": [],
   "source": [
    "text = \"기존 가속기 설계 및 운전에 있어서 엄밀하게 취급 되지 않았던, 공간전하 효과(선형 및 비선형), x-y coupling 및  x-z coupling, 급격한 에너지 변화 등을 고려한 빔 물리 이론 모형을 개발하고, 국제 핵융합 재료조사시설(IFMIF) 개발에 적용한다\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IZWN4xX4HXW"
   },
   "source": [
    "한글 자연어처리기 비교\n",
    "\n",
    "https://konlpy.org/ko/latest/morph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0jLcis9W9R5i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기존', '가속기', '설계', '및', '운전', '에', '있', '어서', '엄밀', '하', '게', '취급', '되', '지', '않', '았', '던', ',', '공간', '전하', '효과', '(', '선형', '및', '비선형', ')', ',', 'x', '-', 'y', 'coupling', '및', 'x', '-', 'z', 'coupling', ',', '급격', '하', 'ㄴ', '에너지', '변화', '등', '을', '고려', '하', 'ㄴ', '빔', '물리', '이론', '모형', '을', '개발', '하', '고', ',', '국제', '핵융합', '재료', '조사', '시설', '(', 'IFMIF', ')', '개발', '에', '적용', '하', 'ㄴ다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "tagged = komoran.morphs(text)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기존', '가속기', '설계', '및', '울', 'ㄴ', '전', '에', '있', '어서', '엄밀', '하', '게', '취급', '되', '지', '않', '아ㄴ', ',', '공간전하', '효과(선형', '및', '비선형', '),', 'x', '-', 'y', 'coupling', '및', 'x', '-', 'z', 'coupling', ',', '급격', '하', 'ㄴ', '에너지', '변화', '등', '을', '고', '려', '하', 'ㄴ', '비', 'ㅁ', '물', 'ㄹ', '리', '이론', '모형', '을', '개발', '하고', ',', '국제', '핵융합', '재료조사시설(IFMIF)', '개발', '에', '적용', '하', 'ㄴ다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "tagged = hannanum.morphs(text)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기존', '가속기', '설계', '및', '운전', '에', '있', '어서', '엄밀', '하', '게', '취급', '되', '지', '않', '았', '더', 'ㄴ', ',', '공간', '전하', '효과', '(', '선형', '및', '비선형', ')', ',', 'x-y', 'coupling', '및', 'x-z', 'coupling', ',', '급격', '하', 'ㄴ', '에너지', '변화', '등', '을', '고려', '하', 'ㄴ', '빔', '물리', '이론', '모형', '을', '개발', '하', '고', ',', '국제', '핵융합', '재료', '조사', '시설', '(', 'IFMIF', ')', '개발', '에', '적용', '하', 'ㄴ다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "tagged = kkma.morphs(text)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기존', '가속기', '설계', '및', '운전', '에', '있', '어서', '엄밀', '하', '게', '취급', '되', '지', '않', '았', '던', ',', '공간', '전하', '효과', '(', '선형', '및', '비', '선형', ')', ',', 'x', '-', 'y', 'coupling', '및', 'x', '-', 'z', 'coupling', ',', '급격', '한', '에너지', '변화', '등', '을', '고려', '한', '빔', '물리', '이론', '모형', '을', '개발', '하', '고', ',', '국제', '핵융합', '재료', '조사', '시설', '(', 'IFMIF', ')', '개발', '에', '적용', '한다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "tagged = mecab.morphs(text)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기존', '가속', '기', '설계', '및', '운전', '에', '있어서', '엄밀하게', '취급', '되지', '않았던', ',', '공간', '전하', '효과', '(', '선형', '및', '비선형', '),', 'x', '-', 'y', 'coupling', '및', 'x', '-', 'z', 'coupling', ',', '급격한', '에너지', '변화', '등', '을', '고려', '한', '빔', '물리', '이론', '모형', '을', '개발', '하고', ',', '국제', '핵융합', '재료', '조사', '시설', '(', 'IFMIF', ')', '개발', '에', '적용', '한다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "tagged = okt.morphs(text)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7nyptjunTG"
   },
   "source": [
    "## 2.3 한글 품사 부착 (PoS Tagging)\n",
    "\n",
    "PoS Tag 목록\n",
    "\n",
    "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "y_HshFQu9R5i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('기존', 'NNG'), ('가속기', 'NNG'), ('설계', 'NNG'), ('및', 'MAJ'), ('운전', 'NNG'), ('에', 'JKB'), ('있', 'VV'), ('어서', 'EC'), ('엄밀', 'XR'), ('하', 'XSA'), ('게', 'EC'), ('취급', 'NNG'), ('되', 'VV'), ('지', 'EC'), ('않', 'VX'), ('았', 'EP'), ('던', 'ETM'), (',', 'SC'), ('공간', 'NNG'), ('전하', 'NNG'), ('효과', 'NNG'), ('(', 'SSO'), ('선형', 'NNG'), ('및', 'MAJ'), ('비', 'XPN'), ('선형', 'NNG'), (')', 'SSC'), (',', 'SC'), ('x', 'SL'), ('-', 'SY'), ('y', 'SL'), ('coupling', 'SL'), ('및', 'MAJ'), ('x', 'SL'), ('-', 'SY'), ('z', 'SL'), ('coupling', 'SL'), (',', 'SC'), ('급격', 'XR'), ('한', 'XSA+ETM'), ('에너지', 'NNG'), ('변화', 'NNG'), ('등', 'NNB'), ('을', 'JKO'), ('고려', 'NNG'), ('한', 'XSV+ETM'), ('빔', 'NNG'), ('물리', 'NNG'), ('이론', 'NNG'), ('모형', 'NNG'), ('을', 'JKO'), ('개발', 'NNG'), ('하', 'XSV'), ('고', 'EC'), (',', 'SC'), ('국제', 'NNG'), ('핵융합', 'NNG'), ('재료', 'NNG'), ('조사', 'NNG'), ('시설', 'NNG'), ('(', 'SSO'), ('IFMIF', 'SL'), (')', 'SSC'), ('개발', 'NNG'), ('에', 'JKB'), ('적용', 'NNG'), ('한다', 'XSV+EC')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "tagged = mecab.pos(text)\n",
    "\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZY4s8tbuuXP"
   },
   "source": [
    "## 2.4 불용어(Stopword) 처리\n",
    "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0iVwim3X9R5i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '휴', '아이구', '아이쿠', '아이고', '어', '나', '우리', '저희', '따라', '의해', '을', '를', '에', '의', '가', '으로', '로', '에게', '뿐이다', '의거하여', '근거하여', '입각하여', '기준으로', '예하면', '예를 들면', '예를 들자면', '저', '소인', '소생', '저희', '지말고', '하지마', '하지마라', '다른', '물론', '또한', '그리고', '비길수 없다', '해서는 안된다', '뿐만 아니라', '만이 아니다', '만은 아니다', '막론하고', '관계없이', '그치지 않다', '그러나', '그런데', '하지만', '든간에', '논하지 않다', '따지지 않다', '설사', '비록', '더라도', '아니면', '만 못하다', '하는 편이 낫다', '불문하고', '향하여', '향해서', '향하다', '쪽으로', '틈타', '이용하여', '타다', '오르다', '제외하고', '이 외에', '이 밖에', '하여야', '비로소', '한다면 몰라도', '외에도', '이곳', '여기', '부터', '기점으로', '따라서', '할 생각이다', '하려고하다', '이리하여', '그리하여', '그렇게 함으로써', '하지만', '일때', '할때', '앞에서', '중에서', '보는데서', '으로써', '로써', '까지', '해야한다', '일것이다', '반드시', '할줄알다', '할수있다', '할수있어', '임에 틀림없다', '한다면', '등', '등등', '제', '겨우', '단지', '다만', '할뿐', '딩동', '댕그', '대해서', '대하여', '대하면', '훨씬', '얼마나', '얼마만큼', '얼마큼', '남짓', '여', '얼마간', '약간', '다소', '좀', '조금', '다수', '몇', '얼마', '지만', '하물며', '또한', '그러나', '그렇지만', '하지만', '이외에도', '대해 말하자면', '뿐이다', '다음에', '반대로', '반대로 말하자면', '이와 반대로', '바꾸어서 말하면', '바꾸어서 한다면', '만약', '그렇지않으면', '까악', '툭', '딱', '삐걱거리다', '보드득', '비걱거리다', '꽈당', '응당', '해야한다', '에 가서', '각', '각각', '여러분', '각종', '각자', '제각기', '하도록하다', '와', '과', '그러므로', '그래서', '고로', '한 까닭에', '하기 때문에', '거니와', '이지만', '대하여', '관하여', '관한', '과연', '실로', '아니나다를가', '생각한대로', '진짜로', '한적이있다', '하곤하였다', '하', '하하', '허허', '아하', '거바', '와', '오', '왜', '어째서', '무엇때문에', '어찌', '하겠는가', '무슨', '어디', '어느곳', '더군다나', '하물며', '더욱이는', '어느때', '언제', '야', '이봐', '어이', '여보시오', '흐흐', '흥', '휴', '헉헉', '헐떡헐떡', '영차', '여차', '어기여차', '끙끙', '아야', '앗', '아야', '콸콸', '졸졸', '좍좍', '뚝뚝', '주룩주룩', '솨', '우르르', '그래도', '또', '그리고', '바꾸어말하면', '바꾸어말하자면', '혹은', '혹시', '답다', '및', '그에 따르는', '때가 되어', '즉', '지든지', '설령', '가령', '하더라도', '할지라도', '일지라도', '지든지', '몇', '거의', '하마터면', '인젠', '이젠', '된바에야', '된이상', '만큼\\t어찌됏든', '그위에', '게다가', '점에서 보아', '비추어 보아', '고려하면', '하게될것이다', '일것이다', '비교적', '좀', '보다더', '비하면', '시키다', '하게하다', '할만하다', '의해서', '연이서', '이어서', '잇따라', '뒤따라', '뒤이어', '결국', '의지하여', '기대여', '통하여', '자마자', '더욱더', '불구하고', '얼마든지', '마음대로', '주저하지 않고', '곧', '즉시', '바로', '당장', '하자마자', '밖에 안된다', '하면된다', '그래', '그렇지', '요컨대', '다시 말하자면', '바꿔 말하면', '즉', '구체적으로', '말하자면', '시작하여', '시초에', '이상', '허', '헉', '허걱', '바와같이', '해도좋다', '해도된다', '게다가', '더구나', '하물며', '와르르', '팍', '퍽', '펄렁', '동안', '이래', '하고있었다', '이었다', '에서', '로부터', '까지', '예하면', '했어요', '해요', '함께', '같이', '더불어', '마저', '마저도', '양자', '모두', '습니다', '가까스로', '하려고하다', '즈음하여', '다른', '다른 방면으로', '해봐요', '습니까', '했어요', '말할것도 없고', '무릎쓰고', '개의치않고', '하는것만 못하다', '하는것이 낫다', '매', '매번', '들', '모', '어느것', '어느', '로써', '갖고말하자면', '어디', '어느쪽', '어느것', '어느해', '어느 년도', '라 해도', '언젠가', '어떤것', '어느것', '저기', '저쪽', '저것', '그때', '그럼', '그러면', '요만한걸', '그래', '그때', '저것만큼', '그저', '이르기까지', '할 줄 안다', '할 힘이 있다', '너', '너희', '당신', '어찌', '설마', '차라리', '할지언정', '할지라도', '할망정', '할지언정', '구토하다', '게우다', '토하다', '메쓰겁다', '옆사람', '퉤', '쳇', '의거하여', '근거하여', '의해', '따라', '힘입어', '그', '다음', '버금', '두번째로', '기타', '첫번째로', '나머지는', '그중에서', '견지에서', '형식으로 쓰여', '입장에서', '위해서', '단지', '의해되다', '하도록시키다', '뿐만아니라', '반대로', '전후', '전자', '앞의것', '잠시', '잠깐', '하면서', '그렇지만', '다음에', '그러한즉', '그런즉', '남들', '아무거나', '어찌하든지', '같다', '비슷하다', '예컨대', '이럴정도로', '어떻게', '만약', '만일', '위에서 서술한바와같이', '인 듯하다', '하지 않는다면', '만약에', '무엇', '무슨', '어느', '어떤', '아래윗', '조차', '한데', '그럼에도 불구하고', '여전히', '심지어', '까지도', '조차도', '하지 않도록', '않기 위하여', '때', '시각', '무렵', '시간', '동안', '어때', '어떠한', '하여금', '네', '예', '우선', '누구', '누가 알겠는가', '아무도', '줄은모른다', '줄은 몰랏다', '하는 김에', '겸사겸사', '하는바', '그런 까닭에', '한 이유는', '그러니', '그러니까', '때문에', '그', '너희', '그들', '너희들', '타인', '것', '것들', '너', '위하여', '공동으로', '동시에', '하기 위하여', '어찌하여', '무엇때문에', '붕붕', '윙윙', '나', '우리', '엉엉', '휘익', '윙윙', '오호', '아하', '어쨋든', '만 못하다\\t하기보다는', '차라리', '하는 편이 낫다', '흐흐', '놀라다', '상대적으로 말하자면', '마치', '아니라면', '쉿', '그렇지 않으면', '그렇지 않다면', '안 그러면', '아니었다면', '하든지', '아니면', '이라면', '좋아', '알았어', '하는것도', '그만이다', '어쩔수 없다', '하나', '일', '일반적으로', '일단', '한켠으로는', '오자마자', '이렇게되면', '이와같다면', '전부', '한마디', '한항목', '근거로', '하기에', '아울러', '하지 않도록', '않기 위해서', '이르기까지', '이 되다', '로 인하여', '까닭으로', '이유만으로', '이로 인하여', '그래서', '이 때문에', '그러므로', '그런 까닭에', '알 수 있다', '결론을 낼 수 있다', '으로 인하여', '있다', '어떤것', '관계가 있다', '관련이 있다', '연관되다', '어떤것들', '에 대해', '이리하여', '그리하여', '여부', '하기보다는', '하느니', '하면 할수록', '운운', '이러이러하다', '하구나', '하도다', '다시말하면', '다음으로', '에 있다', '에 달려 있다', '우리', '우리들', '오히려', '하기는한데', '어떻게', '어떻해', '어찌됏어', '어때', '어째서', '본대로', '자', '이', '이쪽', '여기', '이것', '이번', '이렇게말하자면', '이런', '이러한', '이와 같은', '요만큼', '요만한 것', '얼마 안 되는 것', '이만큼', '이 정도의', '이렇게 많은 것', '이와 같다', '이때', '이렇구나', '것과 같이', '끼익', '삐걱', '따위', '와 같은 사람들', '부류의 사람들', '왜냐하면', '중의하나', '오직', '오로지', '에 한하다', '하기만 하면', '도착하다', '까지 미치다', '도달하다', '정도에 이르다', '할 지경이다', '결과에 이르다', '관해서는', '여러분', '하고 있다', '한 후', '혼자', '자기', '자기집', '자신', '우에 종합한것과같이', '총적으로 보면', '총적으로 말하면', '총적으로', '대로 하다', '으로서', '참', '그만이다', '할 따름이다', '쿵', '탕탕', '쾅쾅', '둥둥', '봐', '봐라', '아이야', '아니', '와아', '응', '아이', '참나', '년', '월', '일', '령', '영', '일', '이', '삼', '사', '오', '육', '륙', '칠', '팔', '구', '이천육', '이천칠', '이천팔', '이천구', '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '령', '영']\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open(\"./stopwords.txt\", encoding = \"cp949\") as f:\n",
    "    for line in f:\n",
    "#         print(line)\n",
    "        stopwords.append(line.strip())\n",
    "#         stopwords.append(line)\n",
    "len(stopwords)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = ['EC', 'EP', 'SC', 'JK', 'JKO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(tagged))\n",
    "\n",
    "words = []\n",
    "\n",
    "for tag in tagged:\n",
    "    if tag[0] in stopwords or tag[1] in stop_pos:\n",
    "        continue\n",
    "    \n",
    "    words.append(tag[0])\n",
    "    \n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1BDV2EAzug6"
   },
   "source": [
    "# 2 N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5NYiTLTI9R5j"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'boy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"I am a boy\"\n",
    "tokens = word_tokenize(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'am'), ('am', 'a'), ('a', 'boy')]\n",
      "[('I', 'am', 'a'), ('am', 'a', 'boy')]\n"
     ]
    }
   ],
   "source": [
    "bigram = bigrams(tokens)\n",
    "trigram = ngrams(tokens, 3)\n",
    "print(list(bigram))\n",
    "print(list(trigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<sos>', '<sos>', '청와대'), ('<sos>', '청와대', '핵심관계자'), ('청와대', '핵심관계자', '이날'), ('핵심관계자', '이날', '기자들'), ('이날', '기자들', '윤'), ('기자들', '윤', '전'), ('윤', '전', '총장'), ('전', '총장', '비판'), ('총장', '비판', '청와대'), ('비판', '청와대', '입장'), ('청와대', '입장', '이'), ('입장', '이', '답'), ('이', '답', '<eos>'), ('답', '<eos>', '<eos>')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"청와대 핵심관계자는 이날 기자들과 만나 윤 전 총장의 비판에 대한 청와대의 입장을 묻자 이같이 답했다.\"\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "nouns = hannanum.nouns(sentence)\n",
    "print(list(ngrams(nouns, 3, pad_left=True, pad_right=True, left_pad_symbol='<sos>', right_pad_symbol='<eos>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('청와대', '핵심관계자', '이날'), ('핵심관계자', '이날', '기자들'), ('이날', '기자들', '윤'), ('기자들', '윤', '전'), ('윤', '전', '총장'), ('전', '총장', '비판'), ('총장', '비판', '청와대'), ('비판', '청와대', '입장'), ('청와대', '입장', '이'), ('입장', '이', '답')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(nouns, 3,  left_pad_symbol='<sos>', right_pad_symbol='<eos>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_Practice_Text_Preprocessing2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8996e42b5752b24954ef4ae74940c33bc999c8a2dee14898c711298040aa64c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}